{"train_batch_size": 4, "train_micro_batch_size_per_gpu": 4, "optimizer": {"type": "Adam", "params": {"lr": 0.0002, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"fp16": true, "enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "hysteresis": 2, "min_loss_scale": 1}, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true}, "wall_clock_breakdown": true, "precision": "fp16", "num_layers": 24, "hidden_size": 2048, "num_attention_heads": 16, "seq_length": 1024, "max_position_embeddings": 1024, "norm": "rmsnorm", "rms_norm_epsilon": 1e-06, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global"], "sparsity_config": {}, "make_vocab_size_divisible_by": 1, "activation": "silu", "scaled_upper_triang_masked_softmax_fusion": true, "use_bias_in_norms": false, "use_bias_in_attn_linear": false, "mlp_type": "llama", "lr_decay_style": "cosine", "lr_decay_iters": 320000, "min_lr": 2e-05, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.0002, "tokenizer_type": "SPMTokenizer", "data_path": "/mnt/ssd-1/data/enwik8/enwik8_text_document", "data_impl": "mmap", "save": "/mnt/data_6tb/dpt-1/checkpoints", "config_files": {"1B.yml": "{\n  \"pipe_parallel_size\": 1,\n  \"model_parallel_size\": 1,\n  \"make_vocab_size_divisible_by\": 1,\n\n  # model settings\n  \"num_layers\": 24,\n  \"hidden_size\": 2048,\n  \"num_attention_heads\": 16,\n  \"seq_length\": 1024,\n  \"max_position_embeddings\": 1024,\n  \"pos_emb\": \"rotary\",\n  \"rotary_pct\": 1,\n  \"no_weight_tying\": true,\n  \"gpt_j_residual\": false,\n  \"output_layer_parallelism\": \"column\",\n  \"norm\": \"rmsnorm\",\n  \"rms_norm_epsilon\": 1.0e-6,\n\n  \"scaled_upper_triang_masked_softmax_fusion\": true,\n  \"bias_gelu_fusion\": false,\n  \"use_bias_in_norms\": false,\n  \"use_bias_in_attn_linear\": false,\n  \"mlp_type\": \"llama\",\n  \"activation\": \"silu\",\n\n  # optimizer settings\n  \"optimizer\":\n    {\n      \"type\": \"Adam\",\n      \"params\": { \"lr\": 0.0002, \"betas\": [0.9, 0.95], \"eps\": 1.0e-8 },\n    },\n  \"min_lr\": 0.00002,\n\n  # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\n  \"zero_optimization\":\n    {\n      \"stage\": 1,\n      \"allgather_partitions\": True,\n      \"allgather_bucket_size\": 500000000,\n      \"overlap_comm\": True,\n      \"reduce_scatter\": True,\n      \"reduce_bucket_size\": 500000000,\n      \"contiguous_gradients\": True,\n    },\n\n  # batch / data settings\n  \"train_micro_batch_size_per_gpu\": 4,\n  \"data_impl\": \"mmap\",\n\n  # activation checkpointing\n  \"checkpoint_activations\": true,\n  \"checkpoint_num_layers\": 1,\n  \"partition_activations\": true,\n  \"synchronize_each_layer\": true,\n\n  # regularization\n  \"gradient_clipping\": 1.0,\n  \"weight_decay\": 0.1,\n  \"hidden_dropout\": 0,\n  \"attention_dropout\": 0,\n\n  # precision settings\n  \"fp16\":\n    {\n      \"fp16\": true,\n      \"enabled\": true,\n      \"loss_scale\": 0,\n      \"loss_scale_window\": 1000,\n      \"hysteresis\": 2,\n      \"min_loss_scale\": 1,\n    },\n\n  # misc. training settings\n  \"train_iters\": 320000,\n  \"lr_decay_iters\": 320000,\n  \"distributed_backend\": \"nccl\",\n  \"lr_decay_style\": \"cosine\",\n  \"warmup\": 0.01,\n  \"checkpoint_factor\": 10000,\n  \"checkpoint_scale\": \"linear\",\n  \"eval_interval\": 1000,\n  \"eval_iters\": 10,\n\n  # logging\n  \"log_interval\": 100,\n  \"steps_per_print\": 10,\n  \"keep_last_n_checkpoints\": 4,\n  \"wall_clock_breakdown\": true,\n}\n", "blackknight.yml": "# Data paths and options when using EleutherAI cluster\n{\n  # you may include multiple distinct datasets if desired\n  # \"train_data_paths\": [\"/mnt/data_6tb/danish-text/train-tiny.jsonl.zst\"],\n  # \"valid_data_paths\": [\"/mnt/data_6tb/danish-text/val-tiny.jsonl.zst\"],\n  # \"test_data_paths\": [\"/mnt/ssd-1/data/enwik8/enwik8_test_text_document\"],\n\n  # if using multiple datasets, provide weights for them to be sampled with\n  # \"train-data-weights\": [1., 2.],\n  # \"test-data-weights\": [2., 1.],\n  # \"valid-data-weights\": [0.5, 0.4],\n\n\n  # If you would like the code to create val and test datasets from your training set use the following instead\n  # \"split\" determines the relative size of train, val, and test\n\n  \"split\": [995,4,1],\n  \"data_path\": \"/mnt/ssd-1/data/enwik8/enwik8_text_document\",\n\n  #\"vocab_file\": \"/mnt/ssd-1/data/gpt2-vocab.json\",\n  #\"merge_file\": \"/mnt/ssd-1/data/gpt2-merges.txt\",\n\n  \"tokenizer_type\": \"SPMTokenizer\",\n  \"vocab_file\": \"/mnt/data_6tb/danish-text/dpt1.model\",\n\n  \"save\": \"/mnt/data_6tb/dpt-1/checkpoints\",\n  \"load\": \"/mnt/data_6tb/dpt-1/checkpoints\",\n  \"log_dir\": \"logs\",\n  \"wandb_project\": \"neox\",\n}\n"}, "load": "/mnt/data_6tb/dpt-1/checkpoints", "checkpoint_factor": 10000, "batch_size": 4, "train_iters": 320000, "eval_iters": 10, "keep_last_n_checkpoints": 4, "split": [995, 4, 1], "vocab_file": "/mnt/data_6tb/danish-text/dpt1.model", "attention_dropout": 0, "hidden_dropout": 0, "weight_decay": 0.1, "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "gas": 1, "clip_grad": 1.0, "dynamic_loss_scale": true, "pipe_parallel_size": 1, "world_size": 1, "is_pipe_parallel": true, "log_dir": "logs", "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "user_script": "train.py", "save_iters": [10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000, 150000, 160000, 170000, 180000, 190000, 200000, 210000, 220000, 230000, 240000, 250000, 260000, 270000, 280000, 290000, 300000, 310000], "global_num_gpus": 1}